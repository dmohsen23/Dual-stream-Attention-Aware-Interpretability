# Multimodal-Attention-Aware-Interpretability
_A deep learning framework for interpretable diagnosis of distal myopathy via multimodal attention-aware fusion._

---

## ğŸ“ Overview

**Distal myopathies** are genetically heterogeneous muscle disorders characterized by specific myofiber alterations.  
This repository implements a **Multimodal Attention-Aware Fusion** model that:  
- Fuses **global** (ResNet50) + **local** (BagNet33) contextual information
- Uses the **Attention Gate** mechanism to efficiently fuse global and local contextual information.
- Generates saliency maps for interpretability  
- Evaluates interpretability using functionally grounded approaches: coherence score and incremental deletion.

---

## âœ¨ Features

- ğŸ” **High Accuracy** on BUSI & Distal Myopathy datasets  
- ğŸ§  **Attention-Aware Fusion** for improved interpretability
- ğŸ“Š **Functionally Grounded Metrics**: coherence score, incremental deletion

---

ğŸŒŸ Inspiration
This work builds upon and extends ideas from:

RadFormer â€“ combining transformers with radiology workflows

GitHub: https://github.com/sbasu276/RadFormer

Publication: Sharma et al., â€œRadFormer: Transformer-based Radiology Report Generationâ€ (ScienceDirect)
https://www.sciencedirect.com/science/article/pii/S1361841522003048

Attention-Gated Networks â€“ integrating attention gates into CNNs for medical imaging

GitHub: https://github.com/ozan-oktay/Attention-Gated-Networks

Paper: Oktay et al., â€œAttention Gated Networks: Learning to Leverage Salient Regions in Medical Imagesâ€

