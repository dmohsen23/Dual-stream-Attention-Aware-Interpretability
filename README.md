# Multimodal-Attention-Aware-Interpretability
_A deep learning framework for interpretable diagnosis of distal myopathy via attention-gated multimodal fusion._

---

## ğŸš€ Table of Contents

1. [Overview](#overview)  
2. [Features](#features)  
3. [Installation](#installation)  
4. [Usage](#usage)  
5. [Results](#results)  
6. [Evaluation](#evaluation)  
7. [Contributing](#contributing)  
8. [License](#license)  
9. [References](#references)  

---

## ğŸ“ Overview

**Distal myopathies** are genetically heterogeneous muscle disorders characterized by specific myofiber alterations.  
This repository implements an **Attention-Gated Multimodal Fusion** model that:  
- Fuses **global** (ResNet50) + **local** (BagNet33) features  
- Generates saliency maps for explainability  
- Evaluates interpretability quantitatively & via clinician feedback  

---

## âœ¨ Features

- ğŸ” **High Accuracy** on BUSI & Distal Myopathy datasets  
- ğŸ§  **Attention-Aware Fusion** for improved discrimination  
- ğŸ“Š **Functionally Grounded Metrics**: coherence score, incremental deletion  
- ğŸ‘©â€âš•ï¸ **Application-Grounded Validation**: survey & thematic analysis with radiologists  

---

## ğŸ’» Installation

1. **Clone**  
   ```bash
   git clone https://github.com/<your-username>/Multimodal-Attention-Aware-Interpretability.git
   cd Multimodal-Attention-Aware-Interpretability

